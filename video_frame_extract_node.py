"""
ğŸ‘»å¹»å½±å·¥å…· - è§†é¢‘é¦–å°¾å¸§è·å–èŠ‚ç‚¹
æœ€ç»ˆä¿®å¤ç‰ˆï¼šå…¼å®¹VideoFromFile/VideoFromComponentsï¼Œä¼˜å…ˆè°ƒç”¨å®˜æ–¹æ–¹æ³•è·å–å¸§ï¼Œè§£å†³æ— æœ‰æ•ˆå¸§æ•°æ®å¼‚å¸¸
"""
import cv2
import numpy as np
import torch
import os
import warnings
from typing import Tuple

# å¿½ç•¥æ— å…³è­¦å‘Šï¼Œé¿å…æ—¥å¿—åˆ·å±
warnings.filterwarnings("ignore")

class VideoFrameExtractNode:
    # èŠ‚ç‚¹æ ¸å¿ƒé…ç½®
    CATEGORY = "ğŸ‘»å¹»å½±å·¥å…·"
    FUNCTION = "extract_first_last_frame"
    RETURN_TYPES = ("IMAGE", "IMAGE")
    RETURN_NAMES = ("é¦–å¸§å›¾åƒ", "å°¾å¸§å›¾åƒ")
    OUTPUT_NODE = True  # æ”¯æŒé¢„è§ˆ

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "è§†é¢‘è·¯å¾„": ("STRING", {
                    "default": "",
                    "tooltip": "å¯é€‰-è§†é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆä¼˜å…ˆçº§é«˜äºè§†é¢‘è¾“å…¥ç«¯å£ï¼‰ï¼Œæ”¯æŒmp4/avi/mov/mkv/flvæ ¼å¼"
                })
            },
            "optional": {
                "è§†é¢‘": ("*", {
                    "forceInput": False,
                    "tooltip": "å¯é€‰è¿-ComfyUIè§†é¢‘å¯¹è±¡ï¼ˆVideoFromFile/VideoFromComponents/ndarrayå¸§åºåˆ—ï¼‰"
                })
            }
        }

    def _cv2frame2comfy(self, frame: np.ndarray) -> torch.Tensor:
        """cv2å¸§è½¬ComfyUIæ ‡å‡†IMAGEæ ¼å¼ï¼ˆfloat32/0-1/[1,H,W,3]ï¼‰"""
        if frame is None or frame.size == 0:
            raise Exception("æ— æ³•è·å–æœ‰æ•ˆå¸§æ•°æ®")
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) if frame.shape[-1] == 3 else frame
        frame_norm = frame_rgb.astype(np.float32) / 255.0
        return torch.from_numpy(np.expand_dims(frame_norm, axis=0))

    def _get_video_path(self, video_obj) -> str:
        """ä»VideoFromFileæå–æœ‰æ•ˆè§†é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆå…œåº•é€»è¾‘ï¼‰"""
        video_path = None
        if hasattr(video_obj, 'get_stream_source'):
            try:
                src = video_obj.get_stream_source()
                if isinstance(src, str) and os.path.exists(src) and src.lower().endswith(('.mp4', '.avi', '.mov', '.mkv', '.flv')):
                    video_path = src
            except:
                pass
        if not video_path:
            for attr in dir(video_obj):
                if attr.startswith('_'):
                    continue
                try:
                    val = getattr(video_obj, attr)
                    if isinstance(val, str) and os.path.exists(val) and val.lower().endswith(('.mp4', '.avi', '.mov', '.mkv', '.flv')):
                        video_path = val
                        break
                except:
                    pass
        return video_path

    def _read_cv2_frame(self, video_path: str, frame_idx: int) -> np.ndarray:
        """CV2è¯»å–æŒ‡å®šå¸§ï¼Œé‡è¯•+èµ„æºé‡Šæ”¾ï¼Œè§£å†³IOè¿æ¥å¼‚å¸¸"""
        for retry in range(2):
            cap = None
            try:
                cap = cv2.VideoCapture(video_path)
                if not cap.isOpened():
                    raise Exception("CV2æ— æ³•æ‰“å¼€è§†é¢‘")
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx - 1 if frame_idx > 0 else 0)
                ret, frame = cap.read()
                if ret and frame is not None:
                    return frame
                else:
                    raise Exception(f"è¯»å–å¸§{frame_idx}å¤±è´¥")
            except Exception as e:
                if retry == 1:
                    raise Exception(f"é‡è¯•åä»æ— æ³•è¯»å–å¸§{frame_idx}ï¼š{str(e)}")
            finally:
                if cap:
                    cap.release()
        return None

    def _scan_nested_obj(self, obj, frame_attrs, depth=0):
        """é€’å½’æ‰«æåµŒå¥—å¯¹è±¡ï¼Œæå–å¸§æ•°æ®ï¼ˆæ ¸å¿ƒä¿®å¤ï¼šå¤„ç†å¤šå±‚åµŒå¥—ï¼‰"""
        video_frames = None
        max_depth = 3  # é™åˆ¶é€’å½’æ·±åº¦ï¼Œé¿å…æ­»å¾ªç¯
        if depth > max_depth:
            return None
        
        # ç›´æ¥åŒ¹é…tensor/ndarrayä¸”ç»´åº¦ä¸º4ï¼ˆå¸§æ•°,H,W,3ï¼‰
        if isinstance(obj, (np.ndarray, torch.Tensor)) and len(obj.shape) == 4:
            return obj
        
        # æ‰«æå½“å‰å¯¹è±¡çš„æŒ‡å®šå±æ€§
        for attr in frame_attrs:
            if hasattr(obj, attr):
                try:
                    val = getattr(obj, attr)
                    # åŒ¹é…å¸§æ•°æ®æ ¼å¼
                    if isinstance(val, (np.ndarray, torch.Tensor)) and len(val.shape) == 4:
                        print(f"âœ… ä»åµŒå¥—å±æ€§.{attr}ï¼ˆæ·±åº¦{depth}ï¼‰æå–åˆ°å¸§æ•°æ® | shapeï¼š{val.shape}")
                        return val
                    # éå¸§æ•°æ®ä½†ä¸ºå¯¹è±¡/å­—å…¸ï¼Œç»§ç»­é€’å½’
                    elif isinstance(val, (object, dict)) and not isinstance(val, (str, int, float, bool)):
                        nested_frames = self._scan_nested_obj(val, frame_attrs, depth + 1)
                        if nested_frames is not None:
                            return nested_frames
                except:
                    continue
        
        # å¤„ç†å­—å…¸ç±»å‹çš„åµŒå¥—ï¼ˆéƒ¨åˆ†è‡ªå®šä¹‰å®ç°ä¼šç”¨å­—å…¸å­˜å‚¨componentsï¼‰
        if isinstance(obj, dict):
            for key, val in obj.items():
                if key in frame_attrs:
                    if isinstance(val, (np.ndarray, torch.Tensor)) and len(val.shape) == 4:
                        print(f"âœ… ä»å­—å…¸key.{key}ï¼ˆæ·±åº¦{depth}ï¼‰æå–åˆ°å¸§æ•°æ® | shapeï¼š{val.shape}")
                        return val
                elif isinstance(val, (object, dict)) and not isinstance(val, (str, int, float, bool)):
                    nested_frames = self._scan_nested_obj(val, frame_attrs, depth + 1)
                    if nested_frames is not None:
                        return nested_frames
        return None

    def _extract_from_components(self, comp_obj) -> Tuple[torch.Tensor, torch.Tensor]:
        """ä»VideoFromComponentsæå–å¸§åºåˆ—ï¼šå¼ºåŒ–é€’å½’æ‰«æ+å…¨é“¾è·¯å…œåº•"""
        video_frames = None

        # ====================== ç¬¬ä¸€æ­¥ï¼šä¼˜å…ˆè°ƒç”¨å®˜æ–¹æ–¹æ³•è·å–å¸§ ======================
        try:
            # 1. è·å–æ€»å¸§æ•°ï¼ŒéªŒè¯è§†é¢‘æœ‰æ•ˆæ€§
            total_frames = comp_obj.get_frame_count()
            if total_frames <= 0:
                raise Exception("VideoFromComponentsè¿”å›æ€»å¸§æ•°ä¸º0")
            
            # 2. é€’å½’æ‰«æget_components()è¿”å›å€¼ï¼ˆæ ¸å¿ƒä¿®å¤ï¼šå¤„ç†åµŒå¥—ç»“æ„ï¼‰
            components = comp_obj.get_components()
            if components:
                frame_attrs = ["frames", "frame_data", "video_frames", "video_data", "tensor", "data"]
                video_frames = self._scan_nested_obj(components, frame_attrs)
            
            # 3. å…œåº•1ï¼šè°ƒç”¨get_frameé€å¸§è¯»å–ï¼ˆæ ‡å‡†æ¥å£ï¼‰
            if video_frames is None and hasattr(comp_obj, 'get_frame'):
                print("âš ï¸ æœªä»componentsæå–åˆ°å¸§ï¼Œå°è¯•get_frameé€å¸§è¯»å–")
                frame_list = []
                max_read_frames = min(total_frames, 1000)  # é™åˆ¶æœ€å¤§è¯»å–æ•°ï¼Œé¿å…å¡æ­»
                for idx in range(max_read_frames):
                    frame = comp_obj.get_frame(idx)
                    if frame is None:
                        break  # å¸§è¯»å–å¤±è´¥åˆ™ç»ˆæ­¢
                    # ç»Ÿä¸€æ ¼å¼ä¸ºndarray
                    if isinstance(frame, torch.Tensor):
                        frame = frame.cpu().numpy()
                    if isinstance(frame, np.ndarray) and len(frame.shape) == 3:
                        frame_list.append(frame)
                if frame_list:
                    video_frames = np.stack(frame_list)
                    print(f"âœ… é€å¸§è¯»å–å®Œæˆï¼Œå…±è·å–{len(frame_list)}å¸§ | shapeï¼š{video_frames.shape}")
                else:
                    raise Exception("get_frameé€å¸§è¯»å–æ— æœ‰æ•ˆæ•°æ®")
            
            # 4. å…œåº•2ï¼šå°è¯•__getitem__ç´¢å¼•è¯»å–ï¼ˆå…¼å®¹è‡ªå®šä¹‰å®ç°ï¼‰
            if video_frames is None and hasattr(comp_obj, '__getitem__'):
                print("âš ï¸ get_frameè¯»å–å¤±è´¥ï¼Œå°è¯•__getitem__ç´¢å¼•è¯»å–")
                frame_list = []
                max_read_frames = min(total_frames, 1000)
                for idx in range(max_read_frames):
                    try:
                        frame = comp_obj[idx]
                        if isinstance(frame, torch.Tensor):
                            frame = frame.cpu().numpy()
                        if isinstance(frame, np.ndarray) and len(frame.shape) == 3:
                            frame_list.append(frame)
                        else:
                            break
                    except:
                        break
                if frame_list:
                    video_frames = np.stack(frame_list)
                    print(f"âœ… ç´¢å¼•è¯»å–å®Œæˆï¼Œå…±è·å–{len(frame_list)}å¸§ | shapeï¼š{video_frames.shape}")
                else:
                    raise Exception("__getitem__ç´¢å¼•è¯»å–æ— æœ‰æ•ˆæ•°æ®")

        except Exception as e:
            raise Exception(f"è°ƒç”¨å®˜æ–¹æ–¹æ³•å¤±è´¥ï¼š{str(e)}")

        # ====================== ç¬¬äºŒæ­¥ï¼šå…¨å±æ€§æ‰«æï¼ˆæœ€ç»ˆå…œåº•ï¼‰ ======================
        if video_frames is None:
            frame_candidate_attrs = []
            # é€’å½’æ‰«ææ‰€æœ‰å±æ€§ï¼ˆå«åµŒå¥—ï¼‰ï¼Œä¾¿äºå®šä½å¸§æ•°æ®ï¼ˆç§»é™¤è°ƒè¯•æ—¥å¿—æ‰“å°ï¼‰
            def print_nested_attr(obj, prefix="", depth=0):
                nonlocal frame_candidate_attrs
                if depth > 3:
                    return
                for attr in dir(obj):
                    if attr.startswith('_'):
                        continue
                    try:
                        val = getattr(obj, attr)
                        # ç­›é€‰ç–‘ä¼¼å¸§å±æ€§
                        if (isinstance(val, (np.ndarray, torch.Tensor)) and
                                len(getattr(val, 'shape', [])) in [3, 4]):
                            frame_candidate_attrs.append(f"{prefix.strip()}{attr}")
                            if video_frames is None:
                                video_frames = val
                        # é€’å½’æ‰«æå­å¯¹è±¡
                        if isinstance(val, (object, dict)) and not isinstance(val, (str, int, float, bool)):
                            print_nested_attr(val, prefix + "  â””â”€", depth + 1)
                    except Exception as e:
                        continue

            print_nested_attr(comp_obj)

        # ====================== ç¬¬ä¸‰æ­¥ï¼šæ ¡éªŒä¸è§£æ ======================
        # æ— åŒ¹é…å±æ€§çš„å…œåº•
        if video_frames is None:
            if frame_candidate_attrs:
                raise Exception(f"æœªåŒ¹é…åˆ°æœ‰æ•ˆå¸§åºåˆ—ï¼ç–‘ä¼¼å¸§å±æ€§ï¼š{frame_candidate_attrs}ï¼Œè¯·æ£€æŸ¥ç»´åº¦æ˜¯å¦ä¸º(å¸§æ•°,H,W,3)")
            else:
                raise Exception("æœªæ‰¾åˆ°ä»»ä½•ç–‘ä¼¼å¸§å±æ€§ï¼ŒVideoFromComponentså¯¹è±¡æ— æœ‰æ•ˆå¸§æ•°æ®")

        # æ ¼å¼è½¬æ¢ä¸ç»´åº¦æ ¡éªŒ
        try:
            # tensorè½¬ndarrayï¼ˆCPUï¼‰
            if isinstance(video_frames, torch.Tensor):
                video_frames = video_frames.cpu().numpy()
            # 3ç»´å•å¸§è‡ªåŠ¨è¡¥ä¸º4ç»´å¤šå¸§
            if len(video_frames.shape) == 3 and video_frames.shape[-1] == 3:
                video_frames = np.expand_dims(video_frames, axis=0)
                print(f"âš ï¸ å•å¸§è‡ªåŠ¨è¡¥ä¸º4ç»´ï¼Œæ–°shapeï¼š{video_frames.shape}")
            # å¿…é¡»æ˜¯4ç»´å¸§åºåˆ—ï¼ˆå¸§æ•°,H,W,3ï¼‰
            if len(video_frames.shape) != 4:
                raise Exception(f"å¸§åºåˆ—ç»´åº¦é”™è¯¯ï¼é¢„æœŸ4ç»´(å¸§æ•°,H,W,3)ï¼Œå®é™…{len(video_frames.shape)}ç»´ï¼Œshapeï¼š{video_frames.shape}")
            # æå–é¦–å°¾å¸§
            first_frame = self._cv2frame2comfy(video_frames[0])
            last_frame = self._cv2frame2comfy(video_frames[-1])
            return (first_frame, last_frame)
        except Exception as e:
            raise Exception(f"å¸§åºåˆ—è§£æå¤±è´¥ï¼š{str(e)}")

    def _process_video_path(self, video_path: str) -> Tuple[torch.Tensor, torch.Tensor]:
        """å¤„ç†è§†é¢‘è·¯å¾„è¾“å…¥ï¼Œè¯»å–é¦–å°¾å¸§"""
        if not video_path or not os.path.exists(video_path):
            raise Exception("è§†é¢‘è·¯å¾„æ— æ•ˆæˆ–ä¸å­˜åœ¨")
        if not video_path.lower().endswith(('.mp4', '.avi', '.mov', '.mkv', '.flv')):
            raise Exception("ä¸æ”¯æŒçš„è§†é¢‘æ ¼å¼ï¼Œä»…æ”¯æŒmp4/avi/mov/mkv/flv")
        
        # è·å–æ€»å¸§æ•°
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise Exception("æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶")
        total_frames = max(1, int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))
        cap.release()
        
        # è¯»å–é¦–å°¾å¸§
        first_frame = self._read_cv2_frame(video_path, 0)
        last_frame = self._read_cv2_frame(video_path, max(0, total_frames - 1))
        
        if first_frame is None:
            raise Exception("æ— æ³•è¯»å–è§†é¢‘é¦–å¸§")
        if last_frame is None:
            raise Exception("æ— æ³•è¯»å–è§†é¢‘å°¾å¸§")
            
        return (self._cv2frame2comfy(first_frame), self._cv2frame2comfy(last_frame))

    def extract_first_last_frame(self, è§†é¢‘è·¯å¾„="", è§†é¢‘=None) -> Tuple[torch.Tensor, torch.Tensor]:
        """ä¸»æ‰§è¡Œå‡½æ•°ï¼šå…¨ç±»å‹å…¼å®¹+å…¨é“¾è·¯å¼‚å¸¸å…œåº•"""
        # ä¼˜å…ˆçº§åˆ¤æ–­ï¼šè§†é¢‘è·¯å¾„æœ‰æ•ˆåˆ™ä¼˜å…ˆä½¿ç”¨
        try:
            if è§†é¢‘è·¯å¾„ and os.path.exists(è§†é¢‘è·¯å¾„) and è§†é¢‘è·¯å¾„.lower().endswith(('.mp4', '.avi', '.mov', '.mkv', '.flv')):
                print("âœ… æ£€æµ‹åˆ°æœ‰æ•ˆè§†é¢‘è·¯å¾„ï¼Œä¼˜å…ˆä½¿ç”¨è·¯å¾„è¯»å–")
                return self._process_video_path(è§†é¢‘è·¯å¾„)
        except Exception as e:
            print(f"âš ï¸ è§†é¢‘è·¯å¾„å¤„ç†å¤±è´¥ï¼š{str(e)}ï¼Œå°è¯•ä½¿ç”¨è§†é¢‘è¾“å…¥ç«¯å£æ•°æ®")
        
        # è§†é¢‘è·¯å¾„æ— æ•ˆ/ä¸å­˜åœ¨ï¼Œä½¿ç”¨è§†é¢‘è¾“å…¥ç«¯å£æ•°æ®
        try:
            # æ£€æŸ¥è§†é¢‘è¾“å…¥ç«¯å£æ˜¯å¦æœ‰æ•°æ®
            if è§†é¢‘ is None:
                raise Exception("è§†é¢‘è¾“å…¥ç«¯å£æ— æ•°æ®ä¸”è§†é¢‘è·¯å¾„æ— æ•ˆ")
            
            # æƒ…å†µ1ï¼šç›´æ¥ä¼ å…¥ndarrayå¸§åºåˆ—
            if isinstance(è§†é¢‘, np.ndarray):
                if len(è§†é¢‘.shape) == 3 and è§†é¢‘.shape[-1] == 3:
                    è§†é¢‘ = np.expand_dims(è§†é¢‘, axis=0)
                if len(è§†é¢‘.shape) == 4:
                    first_frame = self._cv2frame2comfy(è§†é¢‘[0])
                    last_frame = self._cv2frame2comfy(è§†é¢‘[-1])
                    return (first_frame, last_frame)
                else:
                    raise Exception(f"ndarrayå¸§ç»´åº¦é”™è¯¯ï¼Œshapeï¼š{è§†é¢‘.shape}ï¼Œé¢„æœŸ4ç»´(å¸§æ•°,H,W,3)")

            # æƒ…å†µ2ï¼šVideoFromComponentså¯¹è±¡ï¼ˆæ ¸å¿ƒï¼šå¼ºåŒ–é€’å½’æ‰«æï¼‰
            video_type = str(type(è§†é¢‘))
            if "VideoFromComponents" in video_type:
                print("âœ… æ£€æµ‹åˆ°VideoFromComponentsï¼Œå¼€å§‹æ‰«æå±æ€§å¹¶è§£æ")
                return self._extract_from_components(è§†é¢‘)

            # æƒ…å†µ3ï¼šVideoFromFileå¯¹è±¡ï¼ˆå…¼å®¹åŸé€»è¾‘ï¼Œä¿®å¤IOå¼‚å¸¸ï¼‰
            elif "VideoFromFile" in video_type:
                print("âœ… æ£€æµ‹åˆ°VideoFromFileï¼Œä½¿ç”¨CV2è¯»å–é¦–å°¾å¸§")
                video_path = self._get_video_path(è§†é¢‘)
                if not video_path or not os.path.exists(video_path):
                    raise Exception("æ— æ³•ä»VideoFromFileè·å–æœ‰æ•ˆè§†é¢‘è·¯å¾„")
                # åŒæºè·å–æ€»å¸§æ•°
                total_frames = è§†é¢‘.get_frame_count() if (hasattr(è§†é¢‘, 'get_frame_count') and è§†é¢‘.get_frame_count() > 0) else 0
                if total_frames <= 0:
                    cap = cv2.VideoCapture(video_path)
                    total_frames = max(1, int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))
                    cap.release()
                # è¯»å–é¦–å°¾å¸§
                first_frame = self._read_cv2_frame(video_path, 0)
                last_frame = self._read_cv2_frame(video_path, max(0, total_frames - 1))
                
                if first_frame is None:
                    raise Exception("æ— æ³•è¯»å–VideoFromFileé¦–å¸§")
                if last_frame is None:
                    raise Exception("æ— æ³•è¯»å–VideoFromFileå°¾å¸§")
                    
                return (self._cv2frame2comfy(first_frame), self._cv2frame2comfy(last_frame))

            # æƒ…å†µ4ï¼šä¸æ”¯æŒçš„ç±»å‹
            else:
                raise Exception(f"ä¸æ”¯æŒçš„è§†é¢‘ç±»å‹ï¼š{video_type}ï¼Œä»…æ”¯æŒVideoFromFile/VideoFromComponents/ndarray")

        # æ•è·æ‰€æœ‰å¼‚å¸¸å¹¶ç»Ÿä¸€æŠ›å‡ºæŒ‡å®šæç¤º
        except Exception as e:
            raise Exception(f"æ— æ³•è·å–é¦–å°¾å¸§ä¿¡æ¯ï¼š{str(e)}")

# èŠ‚ç‚¹æ³¨å†Œï¼ˆä¸__init__.pyä¿æŒä¸€è‡´ï¼‰
NODE_CLASS_MAPPINGS = {
    "VideoFrameExtractNode": VideoFrameExtractNode
}
NODE_DISPLAY_NAME_MAPPINGS = {
    "VideoFrameExtractNode": "è§†é¢‘é¦–å°¾å¸§è·å–"
}